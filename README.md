# TAREA EXPERIENCIAS EN ANALÃTICA E INTELIGENCIA DE NEGOCIOS

**Integrantes**

Yeniffer Andrea CÃ³rdoba

Pablo GÃ³mez

Luz Adriana Yepes

## Nuevo dominio

Para este proyecto decidimos construir un sistema RAG utilizando informaciÃ³n relacionada con la Universidad EAFIT. Pensamos que desarrollar un modelo RAG con informaciÃ³n real de la universidad podrÃ­a ser un primer paso para en un futuro crear un chatbot inteligente que brinde apoyo tanto a estudiantes actuales como a futuros aspirantes sobre distintos elementos de la instituciÃ³n.

La idea es que este chatbot pueda responder preguntas sobre programas acadÃ©micos, horarios, estructura de la universidad, etc., de una manera conversacional.

## PDFs

Para este primer prototipo del RAG, se reemplazaron los PDFs que habÃ­a en el repositorio y se agregaron cinco nuevos PDFS con informaciÃ³n de la universidad:

- EAFIT: Historia y evoluciÃ³n â€“ Explica el origen y crecimiento de la universidad desde su fundaciÃ³n hasta la actualidad.
- Programas acadÃ©micos y escuelas â€“ Resume la oferta de programas y las principales escuelas acadÃ©micas.
- Campus y vida estudiantil â€“ Describe el ambiente del campus y las actividades que enriquecen la experiencia universitaria.
- InvestigaciÃ³n e innovaciÃ³n â€“ Presenta los proyectos y Ã¡reas de investigaciÃ³n en los que EAFIT se destaca.
- InternacionalizaciÃ³n y admisiones â€“ Detalla las oportunidades de intercambio y los procesos de ingreso para nuevos estudiantes.

## Prompt

El prompt original que habÃ­a en el repositorio fue ligeramente adaptado para que sea mÃ¡s orientado a responder preguntas a estudiantes relativos a informaciÃ³n de la universidad:

*Eres un asistente virtual experto en la Universidad EAFIT.*

*Tu tarea es responder preguntas de estudiantes, profesores o aspirantes utilizando exclusivamente la informaciÃ³n contenida en los documentos disponibles.
Responde siempre de forma clara, formal y profesional.*

*Si la informaciÃ³n no se encuentra en los documentos, indÃ­calo de manera explÃ­cita sin inventar respuestas.*

*Pregunta: {question}*

*Contexto: {context}*

## Conjunto de prueba

Se modificÃ³ el JSON del conjunto de prueba por uno con 5 preguntas relativas a la informaciÃ³n disponible de EAFIT en los PDFs cargados:

- Â¿CuÃ¡ndo fue fundada la Universidad EAFIT y con quÃ© propÃ³sito?
La Universidad EAFIT fue fundada en 1960 por un grupo de empresarios antioqueÃ±os con el propÃ³sito de formar profesionales competentes que respondieran a las necesidades del sector productivo.

- Â¿CuÃ¡ntas escuelas tiene la Universidad EAFIT y cuÃ¡les son?
EAFIT cuenta con cinco escuelas principales: IngenierÃ­a, Finanzas, EconomÃ­a y Gobierno, Ciencias, Derecho y Humanidades.

- Â¿QuÃ© caracterÃ­sticas hacen especial al campus de EAFIT?
El campus de EAFIT se destaca por su diseÃ±o sostenible, sus amplias zonas verdes y su ambiente acadÃ©mico que promueve la cultura, el deporte y la vida universitaria activa.

- Â¿En quÃ© Ã¡reas se enfoca la investigaciÃ³n en EAFIT?
EAFIT promueve la investigaciÃ³n en ciencia de datos, inteligencia artificial, sostenibilidad, medio ambiente e innovaciÃ³n tecnolÃ³gica, impulsando proyectos en colaboraciÃ³n con la industria.

- Â¿QuÃ© oportunidades internacionales ofrece la universidad a sus estudiantes?
EAFIT ofrece programas de intercambio acadÃ©mico, dobles titulaciones y convenios con universidades de todo el mundo para fomentar la experiencia internacional de sus estudiantes.


# ğŸ¤– Chatbot GenAI - Caso de Estudio Recursos Humanos

Este proyecto demuestra cÃ³mo construir, evaluar y automatizar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas prÃ¡cticas de **GenAIOps**.

---

## ğŸ§  Caso de Estudio

El chatbot responde preguntas sobre beneficios, polÃ­ticas internas y roles de una empresa ficticia (**Contoso Electronics**), usando como base una colecciÃ³n de documentos PDF internos.

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ main_interface.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_rrhh.txt
â”‚       â””â”€â”€ v2_resumido_directo.txt
â”œâ”€â”€ data/pdfs/                    â† documentos fuente
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â”œâ”€â”€ eval_dataset.json         â† dataset de evaluaciÃ³n
â”‚   â””â”€â”€ eval_dataset.csv
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ eval.yml
â”‚   â””â”€â”€ test.yml
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

### 1. ğŸ§± PreparaciÃ³n del entorno

```bash
git clone https://github.com/darkanita/GenAIOps_Pycon2025 chatbot-genaiops
cd chatbot-genaiops
conda create -n chatbot-genaiops python=3.10 -y
conda activate chatbot-genaiops
pip install -r requirements.txt
cp .env.example .env  # Agrega tu API KEY de OpenAI
```

---

### 2. ğŸ” Ingesta y vectorizaciÃ³n de documentos

Procesa los PDFs y genera el Ã­ndice vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

Esto:
- Divide los documentos en chunks (por defecto `chunk_size=512`, `chunk_overlap=50`)
- Genera embeddings con OpenAI
- Guarda el Ã­ndice vectorial en `vectorstore/`
- Registra los parÃ¡metros en **MLflow**

ğŸ”§ Para personalizar:
```python
save_vectorstore(chunk_size=1024, chunk_overlap=100)
```

â™»ï¸ Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

---

### 3. ğŸ§  ConstrucciÃ³n del pipeline RAG

```python
from app.rag_pipeline import build_chain
chain = build_chain(vectordb, prompt_version="v1_asistente_rrhh")
```

- Soporta mÃºltiples versiones de prompt
- Usa `ConversationalRetrievalChain` con `LangChain` + `OpenAI`

---

### 4. ğŸ’¬ InteracciÃ³n vÃ­a Streamlit

VersiÃ³n bÃ¡sica:
```bash
streamlit run app/ui_streamlit.py
```

VersiÃ³n combinada con mÃ©tricas:
```bash
streamlit run app/main_interface.py
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica de calidad

Ejecuta:

```bash
python app/run_eval.py
```

Esto:
- Usa `tests/eval_dataset.json` como ground truth
- Genera respuestas usando el RAG actual
- EvalÃºa con `LangChain Eval (QAEvalChain)`
- Registra resultados en **MLflow**

---

### 6. ğŸ“ˆ VisualizaciÃ³n de resultados

Dashboard completo:

```bash
streamlit run app/dashboard.py
```

- Tabla con todas las preguntas evaluadas
- GrÃ¡ficos de precisiÃ³n por configuraciÃ³n (`prompt + chunk_size`)
- Filtrado por experimento MLflow

---

### 7. ğŸ” AutomatizaciÃ³n con GitHub Actions

- CI de evaluaciÃ³n: `.github/workflows/eval.yml`
- Test unitarios: `.github/workflows/test.yml`

---

### 8. ğŸ§ª ValidaciÃ³n automatizada

```bash
pytest tests/test_run_eval.py
```

- EvalÃºa que el sistema tenga al menos 80% de precisiÃ³n con el dataset base

---

## ğŸ” Â¿QuÃ© puedes hacer?

- ğŸ’¬ Hacer preguntas al chatbot
- ğŸ” Evaluar diferentes estrategias de chunking y prompts
- ğŸ“Š Comparar desempeÃ±o con mÃ©tricas semÃ¡nticas
- ğŸ§ª Trazar todo en MLflow
- ğŸ”„ Adaptar a otros dominios (legal, salud, educaciÃ³nâ€¦)

---

## âš™ï¸ Stack TecnolÃ³gico

- **OpenAI + LangChain** â€“ LLM + RAG
- **FAISS** â€“ Vectorstore
- **Streamlit** â€“ UI
- **MLflow** â€“ Registro de experimentos
- **LangChain Eval** â€“ EvaluaciÃ³n semÃ¡ntica
- **GitHub Actions** â€“ CI/CD
- **DevContainer** â€“ Desarrollo portable

---

## ğŸ“ DesafÃ­o para estudiantes

ğŸ§© Parte 1: PersonalizaciÃ³n

1. Elige un nuevo dominio
Ejemplos: salud, educaciÃ³n, legal, bancario, etc.

2. Reemplaza los documentos PDF
UbÃ­calos en data/pdfs/.

3. Modifica o crea tus prompts
Edita los archivos en app/prompts/.

4. Crea un conjunto de pruebas
En tests/eval_dataset.json, define preguntas y respuestas esperadas para evaluar a tu chatbot.

âœ… Parte 2: EvaluaciÃ³n AutomÃ¡tica

1. Ejecuta run_eval.py para probar tu sistema actual.
Actualmente, la evaluaciÃ³n estÃ¡ basada en QAEvalChain de LangChain, que devuelve una mÃ©trica binaria: correcto / incorrecto.

ğŸ”§ Parte 3: Â¡Tu reto! (ğŸ‘¨â€ğŸ”¬ nivel investigador)

1. Mejora el sistema de evaluaciÃ³n:

    * Agrega evaluaciÃ³n con LabeledCriteriaEvalChain usando al menos los siguientes criterios:

        * "correctness" â€“ Â¿Es correcta la respuesta?
        * "relevance" â€“ Â¿Es relevante respecto a la pregunta?
        * "coherence" â€“ Â¿EstÃ¡ bien estructurada la respuesta?
        * "toxicity" â€“ Â¿Contiene lenguaje ofensivo o riesgoso?
        * "harmfulness" â€“ Â¿PodrÃ­a causar daÃ±o la informaciÃ³n?

    * Cada criterio debe registrar:

        * Una mÃ©trica en MLflow (score)

    * Y opcionalmente, un razonamiento como artefacto (reasoning)

    ğŸ“š Revisa la [documentaciÃ³n de LabeledCriteriaEvalChain](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html) para implementarlo.

ğŸ“Š Parte 4: Mejora el dashboard

1. Extiende dashboard.py o main_interface.py para visualizar:

    * Las mÃ©tricas por criterio (correctness_score, toxicity_score, etc.).
    * Una opciÃ³n para seleccionar y comparar diferentes criterios en grÃ¡ficos.
    * (Opcional) Razonamientos del modelo como texto.    

ğŸ§ª Parte 5: Presenta y reflexiona
1. Compara configuraciones distintas (chunk size, prompt) y justifica tu selecciÃ³n.
    * Â¿CuÃ¡l configuraciÃ³n genera mejores respuestas?
    * Â¿En quÃ© fallan los modelos? Â¿Fueron tÃ³xicos o incoherentes?
    * Usa evidencias desde MLflow y capturas del dashboard.

ğŸš€ Bonus

- Â¿Te animas a crear un nuevo criterio como "claridad" o "creatividad"? Puedes definirlo tÃº mismo y usarlo con LabeledCriteriaEvalChain.

---

Â¡Listo para ser usado en clase, investigaciÃ³n o producciÃ³n educativa! ğŸš€
