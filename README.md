# TAREA EXPERIENCIAS EN ANALÃTICA E INTELIGENCIA DE NEGOCIOS

**Integrantes**

Yeniffer Andrea CÃ³rdoba

Pablo GÃ³mez

Luz Adriana Yepes

Hola, profesora. Con este **READ ME** buscamos que le sea mÃ¡s fÃ¡cil ver los pasos que realizamos para estructurar nuestro RAG. Si entra a los distintos archivos del repositorio, podrÃ¡ ver que estÃ¡n ya actualizados acorde a nuestro nuevo dominio (los PDFs en "data", los prompts, el conjunto de prueba, la funciÃ³n de evaluaciÃ³n, etc.). Sin embargo, aquÃ­ hicimos una descripciÃ³n mÃ¡s detallada y en el orden adecuado de las tareas realizadas para que pueda entender mejor las modificaciones presentes en el repositorio. Â¡Muchas gracias!


## Nuevo dominio

Para este proyecto decidimos construir un sistema RAG utilizando informaciÃ³n relacionada con la Universidad EAFIT. Pensamos que desarrollar un modelo RAG con informaciÃ³n real de la universidad podrÃ­a ser un primer paso para en un futuro crear un chatbot inteligente que brinde apoyo tanto a estudiantes actuales como a futuros aspirantes sobre distintos elementos de la instituciÃ³n.

La idea es que este chatbot pueda responder preguntas sobre programas acadÃ©micos, horarios, estructura de la universidad, etc., de una manera conversacional.

## PDFs

Para este primer prototipo del RAG, se reemplazaron los PDFs que habÃ­a en el repositorio y se agregaron cinco nuevos PDFS con informaciÃ³n de la universidad:

- EAFIT: Historia y evoluciÃ³n â€“ Explica el origen y crecimiento de la universidad desde su fundaciÃ³n hasta la actualidad.
- Programas acadÃ©micos y escuelas â€“ Resume la oferta de programas y las principales escuelas acadÃ©micas.
- Campus y vida estudiantil â€“ Describe el ambiente del campus y las actividades que enriquecen la experiencia universitaria.
- InvestigaciÃ³n e innovaciÃ³n â€“ Presenta los proyectos y Ã¡reas de investigaciÃ³n en los que EAFIT se destaca.
- InternacionalizaciÃ³n y admisiones â€“ Detalla las oportunidades de intercambio y los procesos de ingreso para nuevos estudiantes.

## Prompt

El prompt original que habÃ­a en el repositorio fue ligeramente adaptado para que sea mÃ¡s orientado a responder preguntas a estudiantes relativos a informaciÃ³n de la universidad:

*Eres un asistente virtual experto en la Universidad EAFIT.*

*Tu tarea es responder preguntas de estudiantes, profesores o aspirantes utilizando exclusivamente la informaciÃ³n contenida en los documentos disponibles.
Responde siempre de forma clara, formal y profesional.*

*Si la informaciÃ³n no se encuentra en los documentos, indÃ­calo de manera explÃ­cita sin inventar respuestas.*

*Pregunta: {question}*

*Contexto: {context}*

## Conjunto de prueba

Se modificÃ³ el JSON del conjunto de prueba por uno con 5 preguntas relativas a la informaciÃ³n disponible de EAFIT en los PDFs cargados:

- **Â¿CuÃ¡ndo fue fundada la Universidad EAFIT y con quÃ© propÃ³sito?**
La Universidad EAFIT fue fundada en 1960 por un grupo de empresarios antioqueÃ±os con el propÃ³sito de formar profesionales competentes que respondieran a las necesidades del sector productivo.

- **Â¿CuÃ¡ntas escuelas tiene la Universidad EAFIT y cuÃ¡les son?**
EAFIT cuenta con cinco escuelas principales: IngenierÃ­a, Finanzas, EconomÃ­a y Gobierno, Ciencias, Derecho y Humanidades.

- **Â¿QuÃ© caracterÃ­sticas hacen especial al campus de EAFIT?**
El campus de EAFIT se destaca por su diseÃ±o sostenible, sus amplias zonas verdes y su ambiente acadÃ©mico que promueve la cultura, el deporte y la vida universitaria activa.

- **Â¿En quÃ© Ã¡reas se enfoca la investigaciÃ³n en EAFIT?**
EAFIT promueve la investigaciÃ³n en ciencia de datos, inteligencia artificial, sostenibilidad, medio ambiente e innovaciÃ³n tecnolÃ³gica, impulsando proyectos en colaboraciÃ³n con la industria.

- **Â¿QuÃ© oportunidades internacionales ofrece la universidad a sus estudiantes?**
EAFIT ofrece programas de intercambio acadÃ©mico, dobles titulaciones y convenios con universidades de todo el mundo para fomentar la experiencia internacional de sus estudiantes.

## EvaluaciÃ³n

Lo primero que hicimos fue mejorar la funciÃ³n de evaluaciÃ³n con las indicaciones dadas por la profesora. AsÃ­ que tomamos el archivo .py que ya habÃ­a y lo personalizamos para este proyecto utilizando las librerÃ­as **mlflow** y **openai** (usando la API KEY que la profe utilizÃ³ en la clase). En la funciÃ³n creada (que se puede ver en la carpeta "tests" de este repo), se usan los siguientes criterios:

def evaluate_rag_response(question, answer, reference=None):

    criterios = {
    
        "correctness": "Â¿La respuesta es correcta con base en la informaciÃ³n disponible?",
        
        "relevance": "Â¿La respuesta es relevante respecto a la pregunta?",
        
        "coherence": "Â¿EstÃ¡ bien estructurada y es fÃ¡cil de entender?",
        
        "toxicity": "Â¿Contiene lenguaje ofensivo o inapropiado?",
        
        "harmfulness": "Â¿PodrÃ­a causar daÃ±o la informaciÃ³n proporcionada?"
        
    }
    

Y se hizo una evaluaciÃ³n con el siguiente ejemplo (que corresponde a la primera pregunta/repuesta del conjunto de prueba:

if __name__ == "__main__":

    question = "Â¿CuÃ¡les son los programas acadÃ©micos principales que ofrece la Universidad EAFIT?"
    
    answer = "EAFIT ofrece programas en ingenierÃ­a, administraciÃ³n, humanidades, derecho y ciencias."
    
    reference = "EAFIT brinda programas acadÃ©micos en ingenierÃ­a, administraciÃ³n, humanidades y ciencias aplicadas."
    

Las mÃ©tricas obtenidas con nuestro modelo son las siguientes:

Resultados de evaluaciÃ³n:

correctness: 4

relevance: 5

coherence: 5

toxicity: 1

harmfulness: 1

Que, a nuestro parecer, son mÃ©tricas bastante buenas. Pues son altas en lo positivo (correcteness, relevance, coherence) y bajas en lo negativo (toxicity, harmfulness).



<img width="765" height="155" alt="Captura de pantalla 2025-10-25 a la(s) 9 44 03â€¯p m" src="https://github.com/user-attachments/assets/40304461-9c9f-45ec-bb8b-81936d663c33" />

## Dashboard

A continuaciÃ³n, se modifica tambiÃ©n la funciÃ³n de dashboard para que se adapte mejor a las mÃ©tricas que estamos haciendo. Esta funciÃ³n hace uso de **streamlit** para abrir en el navegador un dashboard completo que muestre las mÃ©tricas de una forma mÃ¡s interactiva para el usuario.

Se realizaron los siguientes experimentos adicionales (algunos con respuestas no tan acertadas como las del primer ejemplo) para poder ver mejores comparaciones grÃ¡ficas de las mÃ©tricas de los distintos experimentos registrados en MLFlow:

experimentos = [

    {
        "question": "Â¿CuÃ¡les son los programas acadÃ©micos principales que ofrece la Universidad EAFIT?",
        
        "answer": "EAFIT ofrece programas en ingenierÃ­a, administraciÃ³n, humanidades, derecho y ciencias.",
        
        "reference": "EAFIT brinda programas acadÃ©micos en ingenierÃ­a, administraciÃ³n, humanidades y ciencias aplicadas."
        
    },
    
    {
    
        "question": "Â¿CuÃ¡les son los programas acadÃ©micos principales que ofrece la Universidad EAFIT?",
        
        "answer": "La universidad se dedica principalmente a la mÃºsica y las artes escÃ©nicas.",
        
        "reference": "EAFIT brinda programas acadÃ©micos en ingenierÃ­a, administraciÃ³n, humanidades y ciencias aplicadas."
        
    },
    
    {
    
        "question": "Â¿CuÃ¡les son los programas acadÃ©micos principales que ofrece la Universidad EAFIT?",
        
        "answer": "EAFIT ofrece programas en ingenierÃ­a y administraciÃ³n, aunque tambiÃ©n tiene algunas opciones en ciencias y humanidades.",
        
        "reference": "EAFIT brinda programas acadÃ©micos en ingenierÃ­a, administraciÃ³n, humanidades y ciencias aplicadas."
        
    }
    
]

El dashboard de streamlit nos brinda una tabla con las distintas mÃ©tricas obtenidas en los varios experimentos:

<img width="1310" height="489" alt="Captura de pantalla 2025-10-25 a la(s) 10 23 33â€¯p m" src="https://github.com/user-attachments/assets/88bb5622-ae59-472c-ab05-6b76d7b054c5" />

Y nos muestra el valore promedio de cada mÃ©trica:

<img width="1326" height="552" alt="Captura de pantalla 2025-10-25 a la(s) 10 24 03â€¯p m" src="https://github.com/user-attachments/assets/7a87a3b1-cfd3-48d5-b48a-1cfe9590620e" />


# ğŸ¤– Chatbot GenAI - Caso de Estudio Recursos Humanos

Este proyecto demuestra cÃ³mo construir, evaluar y automatizar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas prÃ¡cticas de **GenAIOps**.

---

## ğŸ§  Caso de Estudio

El chatbot responde preguntas sobre beneficios, polÃ­ticas internas y roles de una empresa ficticia (**Contoso Electronics**), usando como base una colecciÃ³n de documentos PDF internos.

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ main_interface.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_rrhh.txt
â”‚       â””â”€â”€ v2_resumido_directo.txt
â”œâ”€â”€ data/pdfs/                    â† documentos fuente
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â”œâ”€â”€ eval_dataset.json         â† dataset de evaluaciÃ³n
â”‚   â””â”€â”€ eval_dataset.csv
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ eval.yml
â”‚   â””â”€â”€ test.yml
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

### 1. ğŸ§± PreparaciÃ³n del entorno

```bash
git clone https://github.com/darkanita/GenAIOps_Pycon2025 chatbot-genaiops
cd chatbot-genaiops
conda create -n chatbot-genaiops python=3.10 -y
conda activate chatbot-genaiops
pip install -r requirements.txt
cp .env.example .env  # Agrega tu API KEY de OpenAI
```

---

### 2. ğŸ” Ingesta y vectorizaciÃ³n de documentos

Procesa los PDFs y genera el Ã­ndice vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

Esto:
- Divide los documentos en chunks (por defecto `chunk_size=512`, `chunk_overlap=50`)
- Genera embeddings con OpenAI
- Guarda el Ã­ndice vectorial en `vectorstore/`
- Registra los parÃ¡metros en **MLflow**

ğŸ”§ Para personalizar:
```python
save_vectorstore(chunk_size=1024, chunk_overlap=100)
```

â™»ï¸ Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

---

### 3. ğŸ§  ConstrucciÃ³n del pipeline RAG

```python
from app.rag_pipeline import build_chain
chain = build_chain(vectordb, prompt_version="v1_asistente_rrhh")
```

- Soporta mÃºltiples versiones de prompt
- Usa `ConversationalRetrievalChain` con `LangChain` + `OpenAI`

---

### 4. ğŸ’¬ InteracciÃ³n vÃ­a Streamlit

VersiÃ³n bÃ¡sica:
```bash
streamlit run app/ui_streamlit.py
```

VersiÃ³n combinada con mÃ©tricas:
```bash
streamlit run app/main_interface.py
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica de calidad

Ejecuta:

```bash
python app/run_eval.py
```

Esto:
- Usa `tests/eval_dataset.json` como ground truth
- Genera respuestas usando el RAG actual
- EvalÃºa con `LangChain Eval (QAEvalChain)`
- Registra resultados en **MLflow**

---

### 6. ğŸ“ˆ VisualizaciÃ³n de resultados

Dashboard completo:

```bash
streamlit run app/dashboard.py
```

- Tabla con todas las preguntas evaluadas
- GrÃ¡ficos de precisiÃ³n por configuraciÃ³n (`prompt + chunk_size`)
- Filtrado por experimento MLflow

---

### 7. ğŸ” AutomatizaciÃ³n con GitHub Actions

- CI de evaluaciÃ³n: `.github/workflows/eval.yml`
- Test unitarios: `.github/workflows/test.yml`

---

### 8. ğŸ§ª ValidaciÃ³n automatizada

```bash
pytest tests/test_run_eval.py
```

- EvalÃºa que el sistema tenga al menos 80% de precisiÃ³n con el dataset base

---

## ğŸ” Â¿QuÃ© puedes hacer?

- ğŸ’¬ Hacer preguntas al chatbot
- ğŸ” Evaluar diferentes estrategias de chunking y prompts
- ğŸ“Š Comparar desempeÃ±o con mÃ©tricas semÃ¡nticas
- ğŸ§ª Trazar todo en MLflow
- ğŸ”„ Adaptar a otros dominios (legal, salud, educaciÃ³nâ€¦)

---

## âš™ï¸ Stack TecnolÃ³gico

- **OpenAI + LangChain** â€“ LLM + RAG
- **FAISS** â€“ Vectorstore
- **Streamlit** â€“ UI
- **MLflow** â€“ Registro de experimentos
- **LangChain Eval** â€“ EvaluaciÃ³n semÃ¡ntica
- **GitHub Actions** â€“ CI/CD
- **DevContainer** â€“ Desarrollo portable

---

## ğŸ“ DesafÃ­o para estudiantes

ğŸ§© Parte 1: PersonalizaciÃ³n

1. Elige un nuevo dominio
Ejemplos: salud, educaciÃ³n, legal, bancario, etc.

2. Reemplaza los documentos PDF
UbÃ­calos en data/pdfs/.

3. Modifica o crea tus prompts
Edita los archivos en app/prompts/.

4. Crea un conjunto de pruebas
En tests/eval_dataset.json, define preguntas y respuestas esperadas para evaluar a tu chatbot.

âœ… Parte 2: EvaluaciÃ³n AutomÃ¡tica

1. Ejecuta run_eval.py para probar tu sistema actual.
Actualmente, la evaluaciÃ³n estÃ¡ basada en QAEvalChain de LangChain, que devuelve una mÃ©trica binaria: correcto / incorrecto.

ğŸ”§ Parte 3: Â¡Tu reto! (ğŸ‘¨â€ğŸ”¬ nivel investigador)

1. Mejora el sistema de evaluaciÃ³n:

    * Agrega evaluaciÃ³n con LabeledCriteriaEvalChain usando al menos los siguientes criterios:

        * "correctness" â€“ Â¿Es correcta la respuesta?
        * "relevance" â€“ Â¿Es relevante respecto a la pregunta?
        * "coherence" â€“ Â¿EstÃ¡ bien estructurada la respuesta?
        * "toxicity" â€“ Â¿Contiene lenguaje ofensivo o riesgoso?
        * "harmfulness" â€“ Â¿PodrÃ­a causar daÃ±o la informaciÃ³n?

    * Cada criterio debe registrar:

        * Una mÃ©trica en MLflow (score)

    * Y opcionalmente, un razonamiento como artefacto (reasoning)

    ğŸ“š Revisa la [documentaciÃ³n de LabeledCriteriaEvalChain](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html) para implementarlo.

ğŸ“Š Parte 4: Mejora el dashboard

1. Extiende dashboard.py o main_interface.py para visualizar:

    * Las mÃ©tricas por criterio (correctness_score, toxicity_score, etc.).
    * Una opciÃ³n para seleccionar y comparar diferentes criterios en grÃ¡ficos.
    * (Opcional) Razonamientos del modelo como texto.    

ğŸ§ª Parte 5: Presenta y reflexiona
1. Compara configuraciones distintas (chunk size, prompt) y justifica tu selecciÃ³n.
    * Â¿CuÃ¡l configuraciÃ³n genera mejores respuestas?
    * Â¿En quÃ© fallan los modelos? Â¿Fueron tÃ³xicos o incoherentes?
    * Usa evidencias desde MLflow y capturas del dashboard.

ğŸš€ Bonus

- Â¿Te animas a crear un nuevo criterio como "claridad" o "creatividad"? Puedes definirlo tÃº mismo y usarlo con LabeledCriteriaEvalChain.

---

Â¡Listo para ser usado en clase, investigaciÃ³n o producciÃ³n educativa! ğŸš€
